{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy.tokens import Span, Token, Doc\n",
    "from spacy.language import Language\n",
    "from spacy.training import Example\n",
    "import json\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Capítulo 1: Encontrando palabras, frases, nombres y conceptos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quiero una dieta baja en sodio!\n"
     ]
    }
   ],
   "source": [
    "nlp = Spanish()\n",
    "\n",
    "doc = nlp(\"Quiero una dieta baja en sodio!\")\n",
    "\n",
    "print(doc.text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Atributos léxicos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, True, False]\n",
      "[False, False, False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "words = [token.is_alpha for token in doc]\n",
    "symbols = [token.is_punct for token in doc]\n",
    "print(words)\n",
    "print(symbols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MODELOS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De acuerdo con la revista Fortune, Apple fue la empresa más admirada en el mundo entre 2008 y 2012.\n",
      "De          ADP       case      \n",
      "acuerdo     NOUN      fixed     \n",
      "con         ADP       fixed     \n",
      "la          DET       det       \n",
      "revista     NOUN      obl       \n",
      "Fortune     PROPN     appos     \n",
      ",           PUNCT     punct     \n",
      "Apple       PROPN     nsubj     \n",
      "fue         AUX       cop       \n",
      "la          DET       det       \n",
      "empresa     NOUN      ROOT      \n",
      "más         ADV       advmod    \n",
      "admirada    ADJ       amod      \n",
      "en          ADP       case      \n",
      "el          DET       det       \n",
      "mundo       NOUN      obl       \n",
      "entre       ADP       case      \n",
      "2008        NOUN      obl       \n",
      "y           CCONJ     cc        \n",
      "2012        NOUN      conj      \n",
      ".           PUNCT     punct     \n",
      "revista Fortune ORG\n",
      "Apple ORG\n",
      "Entidad faltante: 2008 y 2012.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "text = (\n",
    "    \"De acuerdo con la revista Fortune, Apple fue la empresa \"\n",
    "    \"más admirada en el mundo entre 2008 y 2012.\"\n",
    ")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(text)\n",
    "\n",
    "# Imprime en pantalla el texto del documento\n",
    "print(doc.text)\n",
    "\n",
    "for token in doc:\n",
    "    # Obtén el texto del token, el part-of-speech tag y el dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # Esto es solo por formato\n",
    "    print(\"{:<12}{:<10}{:<10}\".format(token_text, token_pos, token_dep))\n",
    "\n",
    "# Itera sobre las entidades predichas\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto de la entidad y su label\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "years = doc[-4:]\n",
    "print(\"Entidad faltante:\", years.text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PATRONES"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados: ['2008', '2012']\n",
      "Resultados: ['revista Fortune', '2008', '2012']\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LIKE_NUM\": True}]\n",
    "matcher.add(\"PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(f'Resultados: {[doc[start:end].text for match_id, start, end in matches]}')\n",
    "\n",
    "pattern = [{\"POS\": \"NOUN\"}, {\"POS\": \"PROPN\"}]\n",
    "matcher.add(\"NOUN_ADJ_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(f'Resultados: {[doc[start:end].text for match_id, start, end in matches]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Capítulo 2: Análisis de datos a gran escala con spaCy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1212018860762523042\n",
      "revista\n"
     ]
    }
   ],
   "source": [
    "revista_hash = doc.vocab.strings[\"revista\"]\n",
    "print(revista_hash)\n",
    "print(doc.vocab.strings[revista_hash])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontré un nombre propio antes de un verbo: revista\n",
      "Encontré un nombre propio antes de un verbo: empresa\n",
      "Encontré un nombre propio antes de un verbo: mundo\n"
     ]
    }
   ],
   "source": [
    "# Itera sobre los tokens\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        if doc[token.i -1].pos_ == \"DET\":\n",
    "            print(\"Encontré un nombre propio antes de un verbo:\", token.text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6222537\n"
     ]
    }
   ],
   "source": [
    "# Carga el modelo es_core_news_md\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"Hoy hice pan de cereal\")\n",
    "\n",
    "# Obtén el vector para el token \"cereal\"\n",
    "# cereal_vector = doc[4].vector\n",
    "# print(cereal_vector)\n",
    "\n",
    "doc = nlp(\"coca-cola y refresco\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Obtén la similitud entre los tokens \"TV\" y \"libros\"\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 40 años\n",
      "PATTERN1 40 aniversario\n",
      "PATTERN2 pac-man Live\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\n",
    "    \"Cuando pac-man debutó en Tokio, en 1980, nadie podría haber predicho \"\n",
    "    \"que se convertiría en el videojuego más exitoso de todos los tiempos. \"\n",
    "    \"Hoy, 40 años después, aun sigue sorprendiendo. Su desarrolladora, \"\n",
    "    \"Bandai Namco, ha anunciado novedades en el marco del aniversario del \"\n",
    "    \"juego. La celebración del 40 aniversario de pac-Man en 2020 incluirá \"\n",
    "    \"el debut de una nueva canción temática, compuesta por el famoso artista \"\n",
    "    \"japonés de Techno Ken Ishii. Además de estas novedades, Bandai Namco \"\n",
    "    \"publicará nuevas versiones del videojuego. La primera será pac-man Live \"\n",
    "    \"Studio, en Twitch, en colaboración con Amazon Games.\"\n",
    ")\n",
    "\n",
    "# Crea los patrones\n",
    "pattern1 = [{\"LIKE_NUM\": True}, {\"POS\": \"NOUN\"}]\n",
    "pattern2 = [{\"LOWER\": \"pac-man\"}, {\"IS_TITLE\": True}]\n",
    "\n",
    "# Inicializa el Matcher y añade los patrones\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n",
    "# Itera sobre los resultados\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Imprime en pantalla el nombre en string del patrón\n",
    "    # y el texto del span encontrado\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Francia, Alemania, Italia, Bélgica, Países Bajos, Luxemburgo]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../resources/notebook/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = Spanish()\n",
    "doc = nlp(\n",
    "    \"La Unión Europea fue fundada por seis países de Europa occidental \"\n",
    "    \"(Francia, Alemania, Italia, Bélgica, Países Bajos, y Luxemburgo) y \"\n",
    "    \"se amplió en seis ocasiones.\"\n",
    ")\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "# Crea objetos Doc patrón y añádelos al matcher\n",
    "# Esta es una versión más rápida de: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "# Llama al matcher sobre el documento de prueba e imprime el resultado en pantalla\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brasil --> Brasil\n",
      "Brasil --> Alemania\n",
      "Brasil --> India\n",
      "Brasil --> Japón\n",
      "segundos --> Japón\n",
      "Japón --> Alemania\n",
      "dos --> Brasil\n",
      "Brasil --> India\n",
      "incluir --> Brasil\n",
      "Brasil --> Alemania\n",
      "Brasil --> India\n",
      "Brasil --> Japón\n",
      "habitualmente --> Egipto\n",
      "Egipto --> Nigeria\n",
      "apoyado --> Estados Unidos\n",
      "membresía --> Japón\n",
      "apoyo --> India\n",
      "apoyan --> Reino Unido\n",
      "Reino --> Francia\n",
      "acceso --> Alemania\n",
      "Alemania --> Brasil\n",
      "Alemania --> India\n",
      "Alemania --> Japón\n",
      "apoyado --> China\n",
      "membresía --> Japón\n",
      "puja --> India\n",
      "apoyo --> Francia\n",
      "Francia --> Rusia\n",
      "Francia --> Reino Unido\n",
      "Francia --> Estados Unidos\n",
      "adquirido --> India\n",
      "expresó --> China\n",
      "expresado --> China\n",
      "candidatura --> India\n",
      "revoca --> India\n",
      "[('Brasil', 'LOC'), ('Alemania', 'LOC'), ('India', 'LOC'), ('Japón', 'LOC'), ('Japón', 'LOC'), ('Alemania', 'LOC'), ('Brasil', 'LOC'), ('India', 'LOC'), ('Brasil', 'LOC'), ('Alemania', 'LOC'), ('India', 'LOC'), ('Japón', 'LOC'), ('Egipto', 'LOC'), ('Nigeria', 'LOC'), ('Estados Unidos', 'LOC'), ('Japón', 'LOC'), ('India', 'LOC'), ('Reino Unido', 'LOC'), ('Francia', 'LOC'), ('Alemania', 'LOC'), ('Brasil', 'LOC'), ('India', 'LOC'), ('Japón', 'LOC'), ('China', 'LOC'), ('Japón', 'LOC'), ('India', 'LOC'), ('Francia', 'LOC'), ('Rusia', 'LOC'), ('Reino Unido', 'LOC'), ('Estados Unidos', 'LOC'), ('India', 'LOC'), ('China', 'LOC'), ('China', 'LOC'), ('India', 'LOC'), ('India', 'LOC')]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../resources/notebook/country_text.txt\", encoding=\"utf8\") as f:\n",
    "    TEXT = f.read()\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Crea un doc y restablece las entidades existentes\n",
    "doc = nlp(TEXT)\n",
    "doc.ents = []\n",
    "\n",
    "# Itera sobre los resultados\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Crea un Span con el label para \"LOC\"\n",
    "    span = Span(doc, start, end, label=\"LOC\")\n",
    "\n",
    "    # Sobrescribe el doc.ents y añade el span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Obtén el token cabeza de la raíz del span\n",
    "    span_root_head = span.root.head\n",
    "\n",
    "    # Imprime en pantalla el texto del token cabeza de\n",
    "    # la raíz del span y el texto del span\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# Imprime en pantalla las entidades del documento\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"LOC\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Capítulo 3: Pipelines de procesamiento"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'morphologizer', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x0000025D99A8C8B0>), ('morphologizer', <spacy.pipeline.morphologizer.Morphologizer object at 0x0000025D99ACC5E0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x0000025D99A64B80>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x0000025D999B2D60>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x0000025D99A74BC0>), ('lemmatizer', <spacy.lang.es.lemmatizer.SpanishLemmatizer object at 0x0000025D99A6D1C0>)]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "print(nlp.pipeline)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Componentes personalizados para el pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['print_length', 'tok2vec', 'morphologizer', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "Este documento tiene 5 tokens.\n"
     ]
    }
   ],
   "source": [
    "@Language.component(\"length_component\")\n",
    "def length_component(doc):\n",
    "    doc_length = len(doc)\n",
    "    print(f\"Este documento tiene {doc_length} tokens.\")\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Añade el componente en el primer lugar del pipeline e imprime\n",
    "# los nombres de los pipes en pantalla\n",
    "nlp.add_pipe(\"length_component\", name=\"print_length\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"Esto es una frase.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [labrador dorado, gato, tortuga, oso de anteojos]\n",
      "['tok2vec', 'morphologizer', 'parser', 'ner', 'animal_matcher', 'attribute_ruler', 'lemmatizer']\n",
      "[('tortuga', 'ANIMAL'), ('oso de anteojos', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define el componente personalizado\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component(doc):\n",
    "    # Aplica el matcher al doc\n",
    "    matches = matcher(doc)\n",
    "    # Crea un Span para cada resultado y asígna el label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Sobrescribe los doc.ents con los spans resultantes\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "animals = [\"labrador dorado\", \"gato\", \"tortuga\", \"oso de anteojos\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Añade el componente al pipeline después del componente \"ner\"\n",
    "nlp.add_pipe(\"animal_component\", name=\"animal_matcher\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Procesa el texto e imprime en pantalla el texto y el label\n",
    "# de los doc.ents\n",
    "doc = nlp(\"Hoy vimos una tortuga y un oso de anteojos en nuestra caminata\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Atributos personalizados"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Vivo', False), ('en', False), ('España', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "nlp = Spanish()\n",
    "\n",
    "# Registra la extensión de atributo del Token, \"is_country\",\n",
    "# con el valor por defecto False\n",
    "Token.set_extension(\"is_country\", default=False)\n",
    "\n",
    "# Procesa el texto y pon True para el atributo \"is_country\"\n",
    "# para el token \"España\"\n",
    "doc = nlp(\"Vivo en España.\")\n",
    "doc[2]._.is_country = True\n",
    "\n",
    "# Imprime en pantalla el texto del token y el atributo \"is_country\"\n",
    "# para todos los tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Define el método\n",
    "def to_html(span, tag):\n",
    "    # Envuelve el texto del span en un HTML tag y devuélvelo\n",
    "    return f\"<{tag}>{span.text}</{tag}>\"\n",
    "\n",
    "\n",
    "# Registra la extensión de propiedad del Span, \"to_html\",\n",
    "# con el método \"to_html\"\n",
    "Span.set_extension(\"to_html\", method=to_html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body>Hola mundo</body>\n"
     ]
    }
   ],
   "source": [
    "# Procesa el texto y llama el método \"to_html\"en el span\n",
    "# con el nombre de tag \"strong\"\n",
    "doc = nlp(\"Hola mundo, esto es una frase.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(\"body\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Bowie https://es.wikipedia.org/w/index.php?search=David_Bowie\n",
      "Alemania https://es.wikipedia.org/w/index.php?search=Alemania\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Obtén la URL de Wikipedia si el span tiene uno de los siguientes labels\n",
    "    if span.label_ in (\"PER\", \"ORG\", \"LOC\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://es.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Añade la extensión del Span, wikipedia_url, usando el getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Antes de finalizar 1976, el interés de David Bowie en la \"\n",
    "    \"floreciente escena musical alemana, le llevó a mudarse a \"\n",
    "    \"Alemania para revitalizar su carrera.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto y la URL de Wikipedia de la entidad\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['countries_matcher']\n",
      "[('República Checa', 'LOC', 'Prague'), ('República Eslovaca', 'LOC', 'Bratislava')]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../resources/notebook/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "with open(\"../resources/notebook/capitals.json\", encoding=\"utf8\") as f:\n",
    "    CAPITALS = json.loads(f.read())\n",
    "\n",
    "nlp = Spanish()\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", None, *list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "@Language.component(\"countries_component\")\n",
    "def countries_component(doc):\n",
    "    # Crea un Span de entidades con el label \"LOC\" para todos los resultados\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label=\"LOC\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Añade el componente al pipeline\n",
    "nlp.add_pipe(\"countries_component\", name=\"countries_matcher\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# El getter que busca el texto del span en un diccionario de ciudades\n",
    "# capitales de países\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Registra la extensión de atributo del Span, \"capital\", con el \n",
    "# getter get_capital\n",
    "Span.set_extension(\"capital\", getter=get_capital)\n",
    "\n",
    "# Procesa el texto e imprime en pantalla el texto de la entidad,\n",
    "# el label y los atributos \"capital\"\n",
    "doc = nlp(\n",
    "    \"La República Checa podría ayudar a la República Eslovaca \"\n",
    "    \"a proteger su espacio aéreo\"\n",
    ")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Procesando streams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['desaprovechar', 'comprar', 'compré', 'lastima', 'amo']\n",
      "['quiere']\n",
      "['amo']\n",
      "['hacerme', 'esperar', 'llegado']\n",
      "['Tengo']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "with open(\"../resources/notebook/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == \"VERB\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Ahora, Walmart, McDonalds, McDonalds) (Dante, Meal, McMenu, Quesoburguesa, McDonalds) (McDonalds,) (Mcdonalds,) (Tengo, McDonalds)\n"
     ]
    }
   ],
   "source": [
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[David Bowie, Angela Merkel, Lady Gaga]\n"
     ]
    }
   ],
   "source": [
    "people = [\"David Bowie\", \"Angela Merkel\", \"Lady Gaga\"]\n",
    "\n",
    "# Crea una lista de patrones para el PhraseMatcher\n",
    "patterns = list(nlp.pipe(people))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "with open(\"../resources/notebook/bookquotes.json\", encoding=\"utf8\") as f:\n",
    "    DATA = json.loads(f.read())\n",
    "\n",
    "nlp = Spanish()\n",
    "\n",
    "# Registra la extensión del Doc, \"author\" (por defecto None)\n",
    "Doc.set_extension(\"author\", default=None)\n",
    "\n",
    "# Registra la extensión del Doc, \"book\" (por defecto None)\n",
    "Doc.set_extension(\"book\", default=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muchos años después, frente al pelotón de fusilamiento, el coronel Aureliano Buendía había de recordar aquella tarde remota en que su padre lo llevó a conocer el hielo.\n",
      " — 'Cien años de soledad' by Gabriel García Márquez\n",
      "\n",
      "Tenía la ternura torpe de quien nunca ha sido amado y debe improvisar.\n",
      " — 'La casa de los espíritus' by Isabel Allende\n",
      "\n",
      "Andábamos sin buscarnos, pero sabiendo que andábamos para encontrarnos.\n",
      " — 'Rayuela' by Julio Cortázar\n",
      "\n",
      "Cuando fui leona nunca recordé, cómo pude un día mariposa ser.\n",
      " — 'Así' by Alfonsina Storni\n",
      "\n",
      "Los dioses hicieron de maíz a las madres y a los padres. Con maíz amarillo y maíz blanco amasaron su carne.\n",
      " — 'Memoria del fuego' by Eduardo Galeano\n",
      "\n",
      "Y no quiero llantos. La muerte hay que mirarla cara a cara. ¡Silencio!\n",
      " — 'La casa de Bernarda Alba' by Federico García Lorca\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Añade los atributos doc._.book y doc._.author desde el contexto\n",
    "    doc._.book = context[\"book\"]\n",
    "    doc._.author = context[\"author\"]\n",
    "\n",
    "    # Imprime en pantalla el texto y los datos del atributo personalizado\n",
    "    print(f\"{doc.text}\\n — '{doc._.book}' by {doc._.author}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick-fil-A', 'es', 'una', 'cadena', 'de', 'restaurantes', 'de', 'comida', 'rápida', 'americana', 'con', 'sede', 'en', 'la', 'ciudad', 'de', 'College', 'Park', ',', 'Georgia', ',', 'especializada', 'en', 'sándwiches', 'de', 'pollo', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A es una cadena de restaurantes de comida rápida \"\n",
    "    \"americana con sede en la ciudad de College Park, Georgia, \"\n",
    "    \"especializada en sándwiches de pollo.\"\n",
    ")\n",
    "\n",
    "# Únicamente convierte el texto en tokens\n",
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A es una cadena de restaurantes de comida rápida \"\n",
    "    \"americana con sede en la ciudad de College Park, Georgia, \"\n",
    "    \"especializada en sándwiches de pollo.\"\n",
    ")\n",
    "\n",
    "# Deshabilita el tagger y el parser\n",
    "with nlp.disable_pipes(\"morphologizer\", \"parser\"):\n",
    "    # Procesa el texto\n",
    "    doc = nlp(text)\n",
    "    # Imprime las entidades del doc en pantalla\n",
    "    print(doc.ents)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Capítulo 4: Entrenando un modelo de red neuronal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[adidas ZX]\n",
      "[adidas ZX]\n",
      "[adidas ZX]\n",
      "[adidas 8000, adidas 4000]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../resources/notebook/adidas.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = Spanish()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Dos tokens que en minúsculas encuentran \"adidas\" y \"zx\"\n",
    "pattern1 = [{\"LOWER\": \"adidas\"}, {\"LOWER\": \"zx\"}]\n",
    "\n",
    "# Token que en minúsculas encuentra \"adidas\" y un dígito\n",
    "pattern2 = [{\"LOWER\": \"adidas\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Añade los patrones al matcher y revisa el resultado\n",
    "matcher.add(\"ROPA\", [pattern1, pattern2])\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([doc[start:end] for match_id, start, end in matcher(doc)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Cómo pre-ordenar los adidas ZX', {'entities': [(21, 30, 'ROPA')]})\n",
      "('Los nuevos adidas ZX vienen en camino', {'entities': [(11, 20, 'ROPA')]})\n",
      "('Debería pagar €200 por un par de adidas ZX?', {'entities': [(33, 42, 'ROPA')]})\n",
      "('Cuál es la diferencia entre los adidas 8000 y los adidas 4000?', {'entities': [(32, 43, 'ROPA'), (50, 61, 'ROPA')]})\n",
      "('Necesito nuevas zapatillas! ¿Qué me recomiendan?', {'entities': []})\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA = []\n",
    "\n",
    "# Crea un objeto Doc para cada texto en TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Encuentra en el doc y crea una lista de los spans resultantes\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # Obtén los tuples (carácter de inicio, carácter del final, label) resultantes\n",
    "    entities = [(span.start_char, span.end_char, \"ROPA\") for span in spans]\n",
    "    # Da formato a los resultados como tuples con (doc.text, entidades)\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # Añade el ejemplo a los datos de entrenamiento\n",
    "    TRAINING_DATA.append(training_example)\n",
    "\n",
    "print(*TRAINING_DATA, sep=\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creación del modelo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Crea un modelo \"es\" en blanco\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "# Crea un nuevo entity recognizer y añádelo al pipeline\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Añade el label \"ROPA\" al entity recognizer\n",
    "ner.add_label(\"ROPA\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 28.144546270370483}\n",
      "{'ner': 1.009810970427832e-08}\n",
      "{'ner': 1.1983537725710967e-09}\n",
      "{'ner': 5.776937901592609e-10}\n",
      "{'ner': 2.2641713326645672e-10}\n",
      "{'ner': 1.9263452288965482e-10}\n",
      "{'ner': 1.57823824010173e-10}\n",
      "{'ner': 9.846804359455153e-11}\n",
      "{'ner': 1.0698315661136817e-10}\n",
      "{'ner': 9.534702815254137e-11}\n"
     ]
    }
   ],
   "source": [
    "with open(\"../resources/notebook/ropa.json\", encoding=\"utf8\") as f:\n",
    "    TRAINING_DATA = json.loads(f.read())\n",
    "\n",
    "# Comienza el entrenamiento\n",
    "nlp.begin_training()\n",
    "optimizer = nlp.initialize()\n",
    "\n",
    "# Haz un loop por 10 iteraciones\n",
    "for itn in range(100):\n",
    "    # Mezcla los datos de entrenamiento\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    # Crea lotes con los ejemplos e itera sobre ellos\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        for text, annotations in batch:\n",
    "            # create Example\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            # Actualiza el modelo\n",
    "            nlp.update([example], losses=losses, sgd=optimizer)\n",
    "\n",
    "    if itn%10 == 0: print(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}